{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c31d26-c55a-4f90-bd87-afc962ace66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Explain the difference between simple linear regression and multiple linear regression. \n",
    "\n",
    "# Provide an example of each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91286c59-059e-45f4-a0ba-3c529bb7a0c2",
   "metadata": {},
   "source": [
    "Simple Linear Regression is a statistical method used to model the relationship between two variables - one independent variable (predictor) and one dependent variable (response). The goal is to find the best-fitting straight line that represents the relationship between the variables. \n",
    "\n",
    "\n",
    "Example of Simple Linear Regression:\n",
    "\n",
    "Suppose we want to predict a student's final exam score (y) based on the number of hours they studied (x).\n",
    "We collect data from several students and find the following relationship:\n",
    "\n",
    "Hours Studied (x)\tExam Score (y)\n",
    "2\t60\n",
    "3\t70\n",
    "4\t75\n",
    "5\t85\n",
    "6\t90\n",
    "\n",
    "\n",
    "\n",
    "Using simple linear regression, we can find the best-fitting line that represents this relationship and use it to predict exam scores for different levels of study hours.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Multiple Linear Regression: Multiple Linear Regression extends the concept of simple linear regression to include more than one independent variable. It models the relationship between a dependent variable and multiple independent variables. \n",
    "\n",
    "\n",
    "Example of Multiple Linear Regression:\n",
    "\n",
    "Example of Multiple Linear Regression:\n",
    "\n",
    "Let's consider a real estate example. We want to predict the price of a house (y) based on its size in square feet (X1),\n",
    "the number of bedrooms  (X2),  and the age of the house in years (X3).\n",
    "\n",
    "\n",
    "Size (x1)\tBedrooms (x2)\tAge (x3)\tPrice (y)\n",
    "1500\t3\t10\t250,000\n",
    "2000\t4\t5\t320,000\n",
    "1800\t3\t12\t280,000\n",
    "2200\t4\t8\t350,000\n",
    "1600\t2\t15\t230,000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Using multiple linear regression, we can find the best-fitting plane in a three-dimensional space that represents the relationship between these variables and use it to predict house prices based on their size, number of bedrooms, and age.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecf25f7-43ad-4aee-ae87-7e60094465bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Discuss the assumptions of linear regression. \n",
    "\n",
    "# How can you check whether these assumptions hold in a given dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facb429d-f2ee-447a-ac9d-acadaa031a39",
   "metadata": {},
   "source": [
    "Assumptions in Regression : \n",
    "Regression is a parametric approach. ‘Parametric’ means it makes assumptions about data for the purpose of analysis. Due to its parametric side, regression is restrictive in nature. It fails to deliver good results with data sets which doesn’t fulfill its assumptions. Therefore, for a successful regression analysis, it’s essential to validate these assumptions.\n",
    "\n",
    "\n",
    "Violations of assumptions of linear regression can lead to biased or inefficient estimates, and it is important to assess and address these violations for accurate and reliable regression results. Assumptions of linear regression include:\n",
    "\n",
    "\n",
    "Linearity: The relationship between the dependent and independent variables is linear.\n",
    "Independence: The observations are independent of each other.\n",
    "Homoscedasticity: The variance of the errors is constant across all levels of the independent variables.\n",
    "Normality: The errors follow a normal distribution.\n",
    "No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "No endogeneity: There is no relationship between the errors and the independent variables.\n",
    "\n",
    "\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables should be linear. \n",
    "\n",
    "Independence of Errors: The errors (residuals) should be independent of each other, meaning that the error of one observation should not provide information about the error of another observation.\n",
    "\n",
    "Normality of Residuals: The residuals should follow a normal distribution. \n",
    "\n",
    "No Multicollinearity: In multiple linear regression, the independent variables should not be highly correlated with each other. High multicollinearity can lead to inflated standard errors and difficulty in interpreting the individual effects of each variable\n",
    "\n",
    "\n",
    "\n",
    "No Outliers: Outliers can strongly influence the regression model, leading to biased estimates. It's important to identify and handle outliers appropriately.\n",
    "\n",
    "\n",
    "\n",
    "Checking Assumptions:\n",
    "\n",
    "Linearity: You can create scatter plots of each independent variable against the dependent variable to visually assess linearity. \n",
    "\n",
    "Independence of Errors: This assumption can be assessed using residual plots. I\n",
    "\n",
    "Homoscedasticity: Residual plots can also help with assessing homoscedasticity. \n",
    "\n",
    "Normality of Residuals: A histogram or a normal probability plot of the residuals can provide insight into their distribution\n",
    "\n",
    "No Multicollinearity: Calculate the correlation matrix between independent variables. If correlations are very high (close to 1 or -1), multicollinearity might be an issue.\n",
    "\n",
    "\n",
    "\n",
    "No Outliers: Create a scatter plot of the standardized residuals or leverage values. Points that are far from the main cluster might be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7551a199-8164-47b9-9b5d-eeda070fe56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you interpret the slope and intercept in a linear regression model? \n",
    "\n",
    "# Provide an example using a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054d3ef-f1a7-4b29-a206-d50a86043129",
   "metadata": {},
   "source": [
    "Intercept (b0):\n",
    "The intercept represents the estimated value of the dependent variable when the independent variable(s) are zero.\n",
    "\n",
    "\n",
    "Slope (b1):\n",
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider a real-world scenario where we want to predict a person's monthly electricity bill (y) based on the number of kilowatt-hours (kWh) they consumed (x).\n",
    "\n",
    "We use linear regression to model this relationship and obtain the following equation:\n",
    "y=50+0.1x+ε\n",
    "\n",
    "Here, \n",
    "y is the monthly bill, \n",
    "x is the number of kWh consumed, and 50 is the intercept, and 0.1 is the slope.\n",
    "\n",
    "Intercept (): When a person consumes zero kWh (which is unlikely and not practical), the predicted monthly bill would be $50.This intercept is the base cost that the person would need to pay even if they don't use any electricity.\n",
    "\n",
    "Slope (): For each additional kWh consumed, the monthly bill increases by $0.1.\n",
    "\n",
    "\n",
    "\n",
    "For example, if a person consumed 300 kWh in a month, we can predict their monthly bill using the equation:\n",
    "y=50+0.1×300=50+30=80\n",
    "\n",
    "\n",
    "So, based on the model, we would predict that their monthly bill would be $80.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda2302-4f9e-4a78-86f3-5a1137afba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961498d3-8f8a-48ee-807d-4b6352d853f8",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize (or maximize) a function iteratively. It's a widely used technique in machine learning for training models, especially when dealing with models that have adjustable parameters (weights and biases) and an associated cost or loss function that needs to be minimized.\n",
    "\n",
    "Usage in Machine Learning:\n",
    "\n",
    "Gradient descent is fundamental in training machine learning models, particularly for optimizing the parameters of models like linear regression, logistic regression, neural networks, and more. In these models, the goal is to find the set of parameters that minimizes a cost or loss function, which quantifies how well the model's predictions match the actual data.\n",
    "\n",
    "Gradient descent is a key optimization technique used in machine learning to iteratively update model parameters and minimize a cost function, enabling models to learn from data and make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccbdabb-eff8-4d9b-98fc-b129e8ed8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8bc8a0-1b8c-47db-a869-ec104c887484",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is an extension of simple linear regression that allows for the modeling of relationships between a dependent variable and multiple independent variables\n",
    "\n",
    "In simple linear regression, there is only one independent variable, whereas in multiple linear regression, there are two or more independent variables.\n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9786a5-f2e0-4590-b31c-a399f98caa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Explain the concept of multicollinearity in multiple linear regression. \n",
    "# How can you detect and address this issue?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a9acb7-6ffe-4b57-9010-7db89fd13413",
   "metadata": {},
   "source": [
    "Multicollinearity is a phenomenon in multiple linear regression where two or more independent variables in the model are highly correlated with each other. In other words, there is a strong linear relationship between at least two of the independent variables. Multicollinearity can create problems in the regression analysis and affect the stability and interpretability of the model's coefficients.\n",
    "\n",
    "\n",
    "Detecting Multicollinearity:\n",
    "Correlation Matrix: Calculate the correlation coefficients between all pairs of independent variables. \n",
    "\n",
    "\n",
    "\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Feature Selection: If multicollinearity is detected, consider removing one or more of the highly correlated variables from the model. However, be cautious not to remove variables that are theoretically important or necessary for the analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedea3c-002c-4ba5-a330-17b00fe549e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb9054-37ba-4fa9-ae80-bd38d02ab34b",
   "metadata": {},
   "source": [
    "Polynomial Regression is a type of regression analysis that extends the idea of linear regression by introducing polynomial terms as predictors. In polynomial regression, the relationship between the independent variable(s) and the dependent variable is modeled as an nth-degree polynomial equation. This allows the model to capture more complex and nonlinear relationships between the variables.\n",
    "\n",
    "A simple linear regression algorithm only works when the relationship between the data is linear. But suppose we have non-linear data, then linear regression will not be able to draw a best-fit line. Simple regression analysis fails in such conditions.\n",
    "\n",
    "\n",
    "\n",
    "Polynomial regression is a form of Linear regression where only due to the Non-linear relationship between dependent and independent variables, we add some polynomial terms to linear regression to convert it into Polynomial regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea98a4e8-fdba-4f53-999f-a5cbcaa636d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? \\\n",
    "\n",
    "# In what situations would you prefer to use polynomial regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbdf9a7-b02a-4c2e-98b3-1bbdd6a3a2ff",
   "metadata": {},
   "source": [
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Polynomial regression can capture complex relationships and patterns that linear regression cannot. It can fit curves, bends, and nonlinear trends in data.\n",
    "\n",
    "Better Fit: When the relationship between the variables is nonlinear, polynomial regression can provide a better fit to the data, leading to more accurate predictions.\n",
    "\n",
    "\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: With higher-degree polynomials, there's a risk of overfitting the model to noise in the data, resulting in poor generalization to new, unseen data.\n",
    "\n",
    "Complexity: Higher-degree polynomials introduce more parameters into the model, making it more complex and harder to interpret.\n",
    "\n",
    "\n",
    "When to Prefer Polynomial Regression:\n",
    "\n",
    "Polynomial regression is a valuable tool when the relationship between the dependent and independent variables is nonlinear and simple linear regression is not sufficient to capture the pattern.\n",
    "\n",
    "Curved Relationships: When you visually observe that the scatter plot of data suggests a curved or nonlinear relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712da835-4d5e-430f-95d4-8438dff6c871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
