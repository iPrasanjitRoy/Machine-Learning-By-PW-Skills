{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93222510-a5c1-4842-8b6b-ab7f9828b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Lasso Regression, and how does it differ from other regression techniques? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f79b0fa-ce76-49a5-a23f-e6d9b7eb9349",
   "metadata": {},
   "source": [
    "Lasso Regression is a type of linear regression that helps prevent overfitting and performs feature selection by making some coefficients exactly zero. This makes the model simpler and easier to interpret. Unlike other methods, it can choose the most important predictors and ignore less relevant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85507c6a-7a0c-4925-a73a-8078c974c5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the main advantage of using Lasso Regression in feature selection? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac08e72-d0a1-49ea-b9b5-9502bcf0f150",
   "metadata": {},
   "source": [
    "The main advantage of using Lasso Regression for feature selection is that it automatically chooses and keeps the most important predictors while removing the less important ones. This simplifies the model, prevents overfitting, and improves its accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "929edeaa-00b0-4e98-93f5-8de41b3041d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How do you interpret the coefficients of a Lasso Regression model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f86c1-6b35-4a8d-ae75-ddeb5453c77e",
   "metadata": {},
   "source": [
    "Interpreting the coefficients of a Lasso Regression model is similar to interpreting coefficients in other regression techniques, but with an important distinction due to Lasso's feature selection property.\n",
    "\n",
    "\n",
    "Magnitude: Just like in other regressions, the magnitude of a coefficient indicates the strength of the relationship between a predictor variable and the target variable. Larger coefficients suggest a stronger influence.\n",
    "\n",
    "\n",
    "Direction: The sign (positive or negative) of a coefficient still indicates the direction of the relationship. A positive coefficient means that as the predictor variable increases, the target variable tends to increase as well, and vice versa for negative coefficients.\n",
    "\n",
    "\n",
    "Comparative Interpretation: Compare the sizes of coefficients to see which predictors have a larger impact relative to others. Bigger coefficients suggest more influential predictors.\n",
    "\n",
    "Feature Importance: Lasso's distinct property is that it can make some coefficients exactly zero. If a coefficient is zero, the corresponding predictor has been excluded from the model, indicating it's deemed unimportant by Lasso. Non-zero coefficients are the ones that Lasso has selected as meaningful.\n",
    "\n",
    "\n",
    "Lasso can make some coefficients exactly zero, meaning those predictors don't matter in the model. Non-zero coefficients are the important ones that Lasso selected.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7661c4e-baa0-4cbe-b5f9-a58c07e661e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61622ae9-d890-4851-aafe-0e04c10a1316",
   "metadata": {},
   "source": [
    "In Lasso Regression, there's a single tuning parameter called \"alpha.\" It affects how much the model selects important predictors and shrinks coefficients.\n",
    "\n",
    "\n",
    "\n",
    "When alpha is 0, it's like regular linear regression (no selection).\n",
    "\n",
    "\n",
    "When alpha is 1, Lasso strongly selects important predictors and can make some coefficients exactly zero.\n",
    "\n",
    "Values between 0 and 1 balance between selection and shrinking (called Elastic Net).\n",
    "\n",
    "Choosing the right alpha is important for finding the best trade-off between simple models and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7702a9e4-04c0-4564-a9da-5d36d64deefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d5956-9a20-4861-a8e1-33926bbf39b9",
   "metadata": {},
   "source": [
    "Lasso Regression is primarily designed for linear regression problems, meaning it assumes a linear relationship between predictors and the target variable.\n",
    "\n",
    "However, with some modifications, it can also be used for non-linear regression problems by introducing non-linear transformations of the predictors.\n",
    "\n",
    "Here's how you can adapt Lasso Regression for non-linear regression problems:\n",
    "\n",
    "Make Data Twisty: To make Lasso work with non-linear stuff, you can change the way you look at your data. Like, you can turn simple numbers into more complex ones by squaring or multiplying them together.\n",
    "\n",
    "Interaction Terms: Another trick is having predictors cooperate. This means you can multiply predictors together to capture more complex relationships. \n",
    "\n",
    "Remember, while Lasso can handle some non-linear problems, it might struggle with really complex patterns. That's when other methods like decision trees or neural networks step in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea76995-b3bb-4f56-be1f-cd3660a8ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What is the difference between Ridge Regression and Lasso Regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d99d74-4500-4033-9504-ae64b996096c",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are both methods to improve linear regression, but they work a bit differently:\n",
    "\n",
    "Ridge: Makes coefficients smaller but not exactly zero. It's good when you have many predictors and they're related.\n",
    "\n",
    "Lasso: Makes some coefficients exactly zero, so it selects important predictors and can ignore less important ones. It's useful when you want a simple model or need to pick the most important predictors.\n",
    "\n",
    "\n",
    "So, Ridge makes everything smaller, while Lasso can throw away some things altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0579eefe-1b7c-4bbd-aacb-a5394c7246b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791d44b3-da45-480c-b3d8-fc3c29dffe82",
   "metadata": {},
   "source": [
    "Yes, Lasso Regression can help with multicollinearity:\n",
    "\n",
    "Feature Selection: Lasso can ignore some predictors by setting their coefficients to zero. So, it naturally picks one when you have highly related predictors.\n",
    "\n",
    "Simpler Model: With fewer predictors, the model becomes simpler and less affected by multicollinearity problems.\n",
    "\n",
    "However, it might not completely fix everything if multicollinearity is very strong. It's like solving part of the problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa438247-aaa5-47a0-adbb-63e0ab77cc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc91df96-2953-4623-8a51-b7a7804abd85",
   "metadata": {},
   "source": [
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, you typically use techniques like cross-validation. Here's how you can do it:\n",
    "\n",
    "\n",
    "\n",
    "Try Many Lambdas: Test different lambdas, which control how much to shrink coefficients.\n",
    "\n",
    "Use Cross-Validation: Train and test your model on different parts of your data. See which lambda gives the best average performance across these different tests.\n",
    "\n",
    "Choose the Winner: The lambda that makes your model work best on unseen data is the winner. It's the one that strikes the right balance between fitting well and not overcomplicating things.\n",
    "\n",
    "\n",
    "This process ensures that the chosen lambda will make your model perform well on new, unseen data.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
