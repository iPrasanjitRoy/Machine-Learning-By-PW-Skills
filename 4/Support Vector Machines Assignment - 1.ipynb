{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129d4d4-7bab-4fc5-af0e-6947afb28d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the mathematical formula for a linear SVM? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a891dfe-ff7f-4eee-b8fa-131102a525c9",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) can be represented as follows:\n",
    "\n",
    "Given a dataset with N data points, each represented as a feature vector xᵢ of dimensionality D, and corresponding binary class labels yᵢ (where yᵢ is either +1 or -1), a linear SVM seeks to find a hyperplane represented by the equation:\n",
    "\n",
    "w·x + b = 0\n",
    "\n",
    "Here:\n",
    "\n",
    "\"w\" is the weight vector, which is perpendicular to the hyperplane and determines its orientation.\n",
    "\"x\" is the input feature vector.\n",
    "\"b\" is the bias term (also known as the intercept), which shifts the hyperplane away from the origin.\n",
    "\n",
    "\n",
    "The goal of a linear SVM is to find the optimal \"w\" and \"b\" that maximize the margin between the two classes while minimizing classification errors. The margin is defined as the distance between the hyperplane and the nearest data points (support vectors) from each class. \n",
    "\n",
    "\n",
    "The optimization problem for a linear SVM can be formulated as:\n",
    "\n",
    "Minimize: 1/2 ‖w‖²\n",
    "\n",
    "\n",
    "Solving this optimization problem leads to the determination of the optimal \"w\" and \"b\" values, which define the separating hyperplane for the linear SVM. Once these parameters are found, classification of new data points can be performed by evaluating the sign of (w·x + b), where a positive value corresponds to one class, and a negative value corresponds to the other class.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d38f68-a03a-43c0-a368-21a7e14eb36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the objective function of a linear SVM? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ee2fa5-c67e-4a5d-9e1f-07926448f523",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is formulated to find the parameters (weight vector \"w\" and bias term \"b\") that define a hyperplane with the maximum margin between two classes while minimizing classification errors. \n",
    "\n",
    "\n",
    " The primary objective function for a linear SVM is a convex optimization problem. \n",
    " The objective function can be expressed as follows:\n",
    " \n",
    " \n",
    " Minimize: 1/2 ‖w‖²\n",
    "\n",
    " Subject to: yᵢ(w·xᵢ + b) ≥ 1 for all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b604a-2bd1-4f07-82dc-b6fa9e4f0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the kernel trick in SVM? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3dd76-afc0-4663-9ded-c7abbefc7931",
   "metadata": {},
   "source": [
    "The kernel trick in SVM is a technique that allows SVMs to work with non-linear data by transforming it into a higher-dimensional space without explicitly calculating the transformation.\n",
    "\n",
    "\n",
    " Instead, it uses a kernel function to measure similarity in the original space, enabling SVMs to find non-linear decision boundaries. Common kernels include the linear, polynomial, Gaussian (RBF), and sigmoid kernels, each suitable for different types of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d85fd-0d9a-4c01-8304-7a1192c48ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the role of support vectors in SVM Explain with example ? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24dd1b-98f2-4496-ac59-e58c0e943972",
   "metadata": {},
   "source": [
    "Support vectors are the data points closest to the decision boundary in a Support Vector Machine (SVM). They are essential because they define the position and orientation of the decision boundary, ensuring it maximizes the margin between classes while correctly classifying data. These support vectors are critical for the SVM's accuracy and robustness.\n",
    "\n",
    "Here's an explanation with an example:\n",
    "\n",
    "Imagine you have a dataset with two classes, represented by red and blue points on a 2D plane. You want to find a linear SVM classifier that can separate these two classes. The decision boundary (hyperplane) will be the line that best separates the data into two classes.\n",
    "\n",
    "# Step 1: Initially, the SVM algorithm tries to find the widest possible margin, which is the distance between the decision boundary and the nearest data points (support vectors) from each class.\n",
    "\n",
    "# Step 2: Support vectors are identified as the data points that are closest to the decision boundary. In the 2D plane, these support vectors will be the points from each class that are nearest to the decision boundary line. They are essentially the data points that define the margin.\n",
    "\n",
    "# Step 3: The decision boundary is then determined based on these support vectors. It is positioned so that it maximizes the margin while ensuring that it correctly classifies all other data points.\n",
    "\n",
    "# Step 4: Any changes in the position or orientation of the decision boundary depend on the support vectors. The other data points that are not support vectors do not influence the boundary's position, which makes the SVM robust to outliers or noisy data points.\n",
    "\n",
    "In this example, the support vectors are critical because they define the SVM's decision boundary. Their positions ensure that the margin is maximized, and they help the SVM generalize well to new, unseen data points. By focusing on the support vectors, SVM achieves a form of regularization that prevents overfitting and contributes to its ability to handle complex datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c518d6-ddd9-441e-aa86-0d19f4e8cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f5e1a-4118-47ff-b4ed-b3e32bc74593",
   "metadata": {},
   "source": [
    "In SVM, a hyperplane is a decision boundary that separates two classes. In a two-dimensional feature space (2D), it's a straight line; in a higher-dimensional space, it's a hyperplane. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Marginal Plane:\n",
    "\n",
    "A marginal plane, also known as the margin boundary, is the region parallel to the hyperplane that defines the margin\n",
    "\n",
    "\n",
    "\n",
    "Soft Margin:\n",
    "\n",
    "In real-world datasets, perfect separation with a hard margin is often not possible due to noise or overlap between classes. The concept of a soft margin allows for some misclassification to find a better balance between maximizing the margin and minimizing errors.\n",
    "\n",
    "\n",
    "Hard Margin:\n",
    "\n",
    "A hard margin SVM aims to find a hyperplane that perfectly separates the two classes without any misclassification. It requires that all data points are correctly classified and lie outside the margin.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18806e36-802d-495a-90de-72596e97ab90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://editor.analyticsvidhya.com/uploads/20470svm17.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "# Specify The Image File Path\n",
    "# image_path = ''\n",
    "\n",
    "# Display the image\n",
    "# Image(filename=image_path)\n",
    "\n",
    "\n",
    "# Specify The URL Of The Image \n",
    "image_url = 'https://editor.analyticsvidhya.com/uploads/20470svm17.png'\n",
    "\n",
    "# Display The Image From The URL\n",
    "Image(url=image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67b7c93b-e38d-49cb-8d72-c9b72c192f15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://miro.medium.com/v2/resize:fit:2000/1*l5yQxBwZJlwQN3jeGFG5ug.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify The URL Of The Image \n",
    "image_url = 'https://miro.medium.com/v2/resize:fit:2000/1*l5yQxBwZJlwQN3jeGFG5ug.png'\n",
    "\n",
    "# Display The Image From The URL\n",
    "Image(url=image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9123cc-ff23-4e8f-9d33-76ceabb3db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. SVM Implementation through Iris dataset.\n",
    "# Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "# Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "# Compute the accuracy of the model on the testing setl\n",
    "\n",
    "\n",
    "\n",
    "# Plot the decision boundaries of the trained model using two of the featuresl\n",
    "# Try different values of the regularisation parameter C and see how it affects the performance of the model.\n",
    "\n",
    "\n",
    "# Bonus task: Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dba2028b-b6df-4748-9410-20a3a10809cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary libraries \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn import datasets \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.metrics import accuracy_score \n",
    "import pandas as pd \n",
    "\n",
    "# Load The Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "\n",
    "\n",
    "# Create A DataFrame To Display The Data \n",
    "iris_df = pd.DataFrame(data=X, columns=iris.feature_names)\n",
    "iris_df['Target'] = y \n",
    "\n",
    "\n",
    "\n",
    "# Split The Dataset Into A Training Set And A Testing Set (70% Train, 30% Test) \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40) \n",
    "\n",
    "# Train A Linear SVM Classifier \n",
    "svm_classifier = SVC(kernel='linear', C=1.0)  # You can adjust the C parameter\n",
    "svm_classifier.fit(X_train, y_train) \n",
    "\n",
    "# Predict The Labels For The Testing Set \n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Compute The Accuracy Of The Model On The Testing Set \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
