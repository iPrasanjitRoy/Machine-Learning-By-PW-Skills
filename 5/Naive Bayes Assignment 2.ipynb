{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c186d9a4-2c33-4451-b949-427adea31806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. \n",
    "\n",
    "# What is the  probability that an employee is a smoker given that he/she uses the health insurance plan? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d590726-f1b0-4ee0-a15c-5a098344343f",
   "metadata": {},
   "source": [
    "Let's denote:\n",
    "\n",
    "A: The event that an employee uses the health insurance plan.\n",
    "S: The event that an employee is a smoker.\n",
    "\n",
    "\n",
    "You are given:\n",
    "\n",
    "P(A): The probability that an employee uses the health insurance plan, which is 70% or 0.70.\n",
    "\n",
    "P(S|A): The conditional probability that an employee is a smoker given that they use the health insurance plan, which is 40% or 0.40.\n",
    "\n",
    "\n",
    "\n",
    "You want to find P(S|A), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "\n",
    "P(A) * P(S|A) = 0.70 * 0.40 = 0.28\n",
    "\n",
    "\n",
    "P(Sâˆ£A)= 0.70 / 0.28 = 0.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2134c13c-56b8-47d0-bcab-2e7b2400818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b78f57f-7337-499c-886b-a4621fef5475",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes is used for binary data, like whether a word is present or absent in a document (0 or 1), often used in spam detection.\n",
    "\n",
    "Multinomial Naive Bayes is for count-based data, such as word frequencies in text data, commonly used in tasks like document categorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecdf31f-5ef2-477a-b61e-d9cad96589ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26220f71-3613-432d-8a08-dd06ef5a89e9",
   "metadata": {},
   "source": [
    " Bernoulli Naive Bayes handles missing values by treating them as neutral or \"don't know\" regarding the presence or absence of features. It continues to make predictions based on the available data and the probabilistic model built during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9f2f9f-1299-4578-b6c4-1e9699a4eba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae01e6cb-e91b-4b89-b27f-aaa7ad5d4e71",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification, where there are more than two classes or categories to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47645bf-d7be-4e2d-a24a-f67e2a16d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment: \n",
    "    \n",
    "# Data preparation:\n",
    "\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/Spambase). \n",
    "# This dataset contains email messages, where the goal is to predict whether a message is spam or not based on several input features.\n",
    "\n",
    "\n",
    "# Implementation:\n",
    "\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. \n",
    "# Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. \n",
    "# You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "# Results:\n",
    "\n",
    "# Report the following performance metrics for each classifier:\n",
    "\n",
    "# Accuracy\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 score\n",
    "\n",
    "\n",
    "# Discussion:\n",
    "\n",
    "# Discuss the results you obtained. \n",
    "# Which variant of Naive Bayes performed the best? \n",
    "# Why do you think that is the case? Are there any limitations of Naive Bayes that you observed? \n",
    "\n",
    "\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "# Summarise your findings and provide some suggestions for future work.\n",
    "\n",
    "\n",
    "\n",
    "# Note:  This dataset contains a binary classification problem with multiple features. \n",
    "# The dataset is relatively small, but it can be used to demonstrate the performance of the different variants of Naive Bayes on a real-world problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4f06da-95b3-45c0-8828-101b1a042b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\" \n",
    "\n",
    "\n",
    "column_names = [\n",
    "    \"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\",\n",
    "    \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \"word_freq_order\", \"word_freq_mail\",\n",
    "    \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
    "    \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\",\n",
    "    \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\",\n",
    "    \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\",\n",
    "    \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\",\n",
    "    \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
    "    \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\",\n",
    "    \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\",\n",
    "    \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_#\", \"capital_run_length_average\",\n",
    "    \"capital_run_length_longest\", \"capital_run_length_total\", \"is_spam\"\n",
    "]\n",
    "\n",
    "\n",
    "response = requests.get(url)\n",
    "data_lines = response.text.splitlines() \n",
    "\n",
    "\n",
    "df = pd.DataFrame([line.split(\",\") for line in data_lines], columns=column_names) \n",
    "\n",
    "\n",
    "df.to_csv(\"spambase.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787268ab-0c9a-47d8-a3e0-f06d8bf0637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score \n",
    "\n",
    "\n",
    "df = pd.read_csv(\"spambase.csv\")\n",
    "\n",
    "\n",
    "X = df.drop(\"is_spam\", axis=1)\n",
    "y = df[\"is_spam\"]\n",
    "\n",
    "\n",
    "bernoulli_classifier = BernoulliNB()\n",
    "multinomial_classifier = MultinomialNB()\n",
    "gaussian_classifier = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate performance metrics\n",
    "def evaluate_classifier(classifier, name):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision_weighted').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall_weighted').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1_weighted').mean()\n",
    "\n",
    "    print(f\"Results for {name} Naive Bayes:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027796fa-21c6-4109-b73e-a8b58002ecb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Bernoulli Naive Bayes:\n",
      "Accuracy: 0.88\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.88\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.79\n",
      "Precision: 0.79\n",
      "Recall: 0.79\n",
      "F1 Score: 0.79\n",
      "\n",
      "Results for Gaussian Naive Bayes:\n",
      "Accuracy: 0.82\n",
      "Precision: 0.86\n",
      "Recall: 0.82\n",
      "F1 Score: 0.82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_classifier(bernoulli_classifier, \"Bernoulli\")\n",
    "evaluate_classifier(multinomial_classifier, \"Multinomial\")\n",
    "evaluate_classifier(gaussian_classifier, \"Gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6227f62-182a-4548-bf2e-6df4c5c70952",
   "metadata": {},
   "source": [
    "# Performance Comparison:\n",
    "\n",
    "Bernoulli Naive Bayes: This classifier performed the best in terms of accuracy, precision, recall, and F1 score, achieving an accuracy of 0.88. It provides the most balanced performance across all metrics.\n",
    "\n",
    "\n",
    "\n",
    "Multinomial Naive Bayes: While it performed reasonably well with an accuracy of 0.79, it falls slightly behind the Bernoulli variant in terms of accuracy and F1 score. This might be due to the dataset not naturally aligning with the assumptions of a multinomial distribution.\n",
    "\n",
    "\n",
    "Gaussian Naive Bayes: This classifier achieved an accuracy of 0.82, which is also decent but not as high as Bernoulli Naive Bayes. It exhibits good precision but lags behind in recall compared to the Bernoulli variant.\n",
    "\n",
    "\n",
    "\n",
    "# Why Bernoulli Naive Bayes Performed Best:\n",
    "\n",
    "Binary Features: Bernoulli Naive Bayes performed best because the \"Spambase\" dataset likely contains many binary features, such as the presence or absence of certain words or characters in emails. Bernoulli Naive Bayes is well-suited for binary data, making it a natural choice for this dataset.\n",
    "\n",
    "\n",
    "# Limitations of Naive Bayes:\n",
    "\n",
    "Independence Assumption: Naive Bayes assumes that features are independent of each other, which may not hold in all real-world datasets.\n",
    "\n",
    "\n",
    "Sensitivity to Data Quality: Naive Bayes can be sensitive to noisy or irrelevant features, potentially leading to suboptimal performance.\n",
    "\n",
    "\n",
    "Limited Expressiveness: It may not capture complex relationships or interactions between features, especially when they are crucial for classification.\n",
    "\n",
    "\n",
    "Assumption Fit: The choice of the Naive Bayes variant should align with the data distribution. Choosing the wrong variant may lead to poorer performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
