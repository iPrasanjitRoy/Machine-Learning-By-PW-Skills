{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91515e3a-3e0a-4950-9d9c-0d2900d6c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. You are working on a machine learning project where you have a dataset containing numerical and categorical features.\n",
    "\n",
    "# You have identified that some of the features are highly correlated and there are missing values in some of the columns. \n",
    "\n",
    "# You want to build a pipeline that automates the feature engineering process and handles the missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d95863-64e8-4245-9478-04e570c80513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a pipeline that includes the following steps:   \n",
    "# Use an automated feature selection method to identify the important features in the dataset.\n",
    "\n",
    "# Create a numerical pipeline that includes the following steps : \n",
    "# Impute the missing values in the numerical columns using the mean of the column values. \n",
    "# Scale the numerical columns using standardisation. \n",
    "    \n",
    "# Create a categorical pipeline that includes the following steps:  \n",
    "# Impute the missing values in the categorical columns using the most frequent value of the column.  \n",
    "# One-hot encode the categorical columns. \n",
    "    \n",
    "# Combine the numerical and categorical pipelines using a ColumnTransformer. \n",
    "# Use a Random Forest Classifier to build the final model. \n",
    "# Evaluate the accuracy of the model on the test dataset. \n",
    "\n",
    "# Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of each step. \n",
    "# You should also provide an interpretation of the results and suggest possible improvements for the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7c27657-52c5-4448-9b28-10116af1a8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 13932964.31\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Load Your Dataset \n",
    "data = pd.read_csv('Insurance_data.csv')\n",
    "\n",
    "# Separate Target Variable \n",
    "X = data.drop('Charges', axis=1)  \n",
    "y = data['Charges'] \n",
    "\n",
    "# Define Categorical Columns \n",
    "categorical_columns = ['Sex', 'Smoker', 'Region'] \n",
    "\n",
    "# Split Data Into Train And Test Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# Automated Feature Selection with One-Hot Encoding \n",
    "feature_selector = SelectFromModel(RandomForestRegressor(n_estimators=100, random_state=42)) \n",
    "\n",
    "# Define Numerical And Categorical Features \n",
    "numerical_features = X_train.select_dtypes(include=['float64', 'int64']).columns.tolist() \n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist() \n",
    "\n",
    "# Numerical Pipeline \n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical Pipeline \n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine The Numerical And Categorical Pipelines Using ColumnTransformer \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Final Pipeline With Preprocessing And Model \n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', feature_selector),  \n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Fit The Pipeline On The Training Data \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions On The Test Data \n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate The Model (Use Mean Squared Error For Regression Tasks) \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2014393d-596f-43eb-9951-47a8cb005c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import random \n",
    "\n",
    "# Set A Random Seed For Reproducibility \n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate Random Data For The Dataset \n",
    "n_samples = 100\n",
    "\n",
    "# Placeholder List Of Names  \n",
    "names = [\"John\", \"Jane\", \"Alice\", \"Bob\", \"Eva\", \"David\", \"Sophia\", \"Michael\", \"Olivia\", \"William\"]\n",
    "\n",
    "data = {\n",
    "   \n",
    "    'Name': [random.choice(names) for _ in range(n_samples)],\n",
    "    'Age': np.random.randint(18, 65, size=n_samples),\n",
    "    'Sex': np.random.choice(['Male', 'Female'], size=n_samples),\n",
    "    'Education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD']), \n",
    "    'Salary': np.random.randint(20000, 80000, size=n_samples),\n",
    "    'Credit_Score': np.random.randint(300, 850, size=n_samples),\n",
    "    'Loan_Grant': np.random.choice(['Yes', 'No'], size=n_samples)\n",
    "}\n",
    "\n",
    "# Create A Dataframe From The Random Data \n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Save The Dataframe To A CSV File \n",
    "df.to_csv('Loan_Data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40a102ca-67d0-4455-bf73-9a99302a6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.feature_selection import SelectFromModel \n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "\n",
    "# Load The Dataset \n",
    "data = pd.read_csv('Loan_Data.csv')\n",
    "\n",
    "# Separate Target Variable \n",
    "X = data.drop('Loan_Grant', axis=1) \n",
    "y = data['Loan_Grant'] \n",
    "\n",
    "# Drop 'Name' Feature \n",
    "X = X.drop('Name', axis=1)\n",
    "\n",
    "# Define Categorical And Numerical Features \n",
    "categorical_features = ['Sex', 'Education'] \n",
    "numerical_features = ['Age', 'Salary', 'Credit_Score'] \n",
    "\n",
    "# Split The Data Into Train And Test Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f19e179-7293-4b37-bdfd-ef1192a1587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Pipeline \n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical Pipeline \n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77c052ae-829c-4b59-8288-4f7d900fe0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "686cd3f6-b1ac-47ba-9ba9-85acc1fc917b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selector', SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "420a5339-5f54-4c0b-a7e7-f652ab34f8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce4a7144-eb31-4133-bc68-014ab5c26b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee33f6b2-d7f9-427a-85c3-7e744673dbd3",
   "metadata": {},
   "source": [
    "# The SelectFromModel IS A Feature Selection Technique In Scikit-learn That Allows You To Select The Most Important Features From A Dataset Based On The Importance Scores Assigned To Them By A Machine Learning Model. In This Case, I Am Using SelectFromModel With A RandomForestClassifier As The Model To Select Important Features For Classification. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N Estimators = 100: This Specifies The Number Of Decision Trees In The Forest. You Are Using 100 Decision Trees.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65ee1d2a-1f0e-47a1-ad31-e071cc1ee516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. \n",
    "\n",
    "# Train the pipeline on the iris dataset and evaluate its accuracy. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a24a1ec-49e0-4994-9442-08dd2f105858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "# Load The Iris Dataset \n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split The Data Into Training And Testing Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create Individual Classifiers \n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "lr_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Create A Voting Classifier \n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('random_forest', rf_classifier),\n",
    "        ('logistic_regression', lr_classifier)\n",
    "    ],\n",
    "    voting='hard'  # You Can Use 'Soft' For Weighted Voting If Both Classifiers Support Probability Estimates \n",
    ")\n",
    "\n",
    "\n",
    "# Create A Pipeline That Includes Scaling And The Voting Classifier \n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Standardize Features For Logistic Regression \n",
    "    ('voting_classifier', voting_classifier)\n",
    "])\n",
    "\n",
    "# Fit The Pipeline On The Training Data \n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Make Predictions On The Test Data \n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "\n",
    "# Evaluate The Accuracy Of The Ensemble Model \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45d2c37-4d92-4633-8d3b-8ac3726e02b4",
   "metadata": {},
   "source": [
    "#  A Voting Classifier Is An Ensemble Machine Learning Model That Combines The Predictions Of Multiple Individual Classifiers (Estimators) To Make A Final Decision. It Can Be Used For Both Classification And Regression Tasks. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
