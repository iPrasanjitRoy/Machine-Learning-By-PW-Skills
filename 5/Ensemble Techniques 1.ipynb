{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb1bda4-d39e-4eee-bf5f-9af30e010ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is an ensemble technique in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ad556-c029-474d-9de6-45b08b0a1e71",
   "metadata": {},
   "source": [
    "An ensemble technique in machine learning is a method that combines predictions from multiple models to make more accurate predictions. It's like asking multiple experts for their opinions and combining them to get a better result. Ensembles are used to improve the accuracy and robustness of machine learning models.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db953ef-4bfa-417e-9c80-9cf23eb3c036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Why are ensemble techniques used in machine learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d7095a-854b-48c8-9f61-f7abd14a21e0",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several key reasons:\n",
    "\n",
    "Improved Accuracy: Ensembles can significantly enhance predictive accuracy by combining the strengths of multiple base models. They often outperform individual models, especially when the base models have diverse characteristics.\n",
    "\n",
    "\n",
    "\n",
    "Reduction of Overfitting: Ensembles can reduce the risk of overfitting, where a model performs well on the training data but poorly on unseen data. By combining multiple models with different biases, ensembles tend to generalize better.\n",
    "\n",
    "Ensemble techniques are used in machine learning to improve prediction accuracy, reduce overfitting, and make models more robust by combining the predictions of multiple individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1083d2-20f4-4ba7-8012-04db8ced8a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is bagging? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e16353-2f14-4d4f-b6e8-475fbcd672e7",
   "metadata": {},
   "source": [
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble machine learning technique that aims to improve the performance and robustness of predictive models.\n",
    "\n",
    "Bagging, short for Bootstrap Aggregating, is a machine learning technique that creates multiple models by training on random subsets of the training data and then combines their predictions to improve accuracy and reduce overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767d500-4e26-426a-b937-0a41a1cafbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is boosting? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad098757-91e8-4941-a2a5-fe75ac5f5250",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that aims to enhance the performance of predictive models by combining the predictions of multiple weak learners (typically simple models) in a sequential and adaptive manner. The primary goal of boosting is to give more weight or importance to data points that are challenging to predict correctly, thereby improving overall accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a36f30-1a32-4f28-96a0-ca92523d2e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What are the benefits of using ensemble techniques?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed21157-551e-4b96-84f3-dabd4b138770",
   "metadata": {},
   "source": [
    "Using ensemble techniques in machine learning offers several key benefits:\n",
    "\n",
    "\n",
    "Improved Accuracy: Ensemble methods can significantly enhance predictive accuracy by combining the strengths of multiple base models. This often results in better overall performance compared to individual models.\n",
    "\n",
    "\n",
    "Reduction of Overfitting: Ensembles can help reduce the risk of overfitting, where a model performs well on the training data but poorly on unseen data. By combining multiple models with different biases, ensembles tend to generalize better.\n",
    "\n",
    "\n",
    "Robustness: Ensembles are more robust to noisy or outlier data points. Individual models may make errors on certain examples, but ensembles can mitigate these errors through aggregation.\n",
    "\n",
    "\n",
    "Enhanced Stability: Ensembles tend to have lower variance in their predictions compared to individual models, making them more stable and reliable.\n",
    "\n",
    "\n",
    "Model Interpretability: In some cases, ensembles can provide insights into feature importance and model behavior, making them valuable for understanding the underlying relationships in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef07ec6-a4ce-4883-bbbd-a61affc71701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Are ensemble techniques always better than individual models? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aabfb5-5981-4254-be92-89f6d1d6f49d",
   "metadata": {},
   "source": [
    "Ensemble techniques are not always better than individual models; their effectiveness depends on several factors and the specific problem at hand. \n",
    "\n",
    "When Ensembles May Not Be Preferred:\n",
    "\n",
    "Computational Cost: Ensembles can be computationally expensive, as they require training and maintaining multiple models. In some applications with strict computational constraints, using a single well-tuned model may be preferred.\n",
    "\n",
    "\n",
    "Data Limitations: In cases where the available dataset is small, noisy, or lacks diversity, ensembles may not provide significant benefits. Individual models might perform well with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ada634d-4b2e-482f-94f5-253d8351799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. How is the confidence interval calculated using bootstrap? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c25b3b-077e-4ea8-af1e-c79d5b3ea9ed",
   "metadata": {},
   "source": [
    "Resampling: In the bootstrap method, you start with your original dataset and repeatedly create new datasets by randomly selecting data points from the original data, allowing for replacement. This creates multiple resampled datasets.\n",
    "\n",
    "\n",
    "Statistic Computation: For each of these resampled datasets, you calculate the statistic of interest. This could be the mean, median, standard deviation, or any other statistic you want to estimate.\n",
    "\n",
    "\n",
    "Distribution of Statistics: You end up with a distribution of these statistics, which represents the variability in the statistic due to random sampling.\n",
    "\n",
    "Percentiles: To create a confidence interval, you look at percentiles of this distribution. For example, for a 95% confidence interval, you find the 2.5th percentile (lower bound) and the 97.5th percentile (upper bound) of the statistic distribution.\n",
    "\n",
    "\n",
    "\n",
    "Interpretation: The resulting range between the lower and upper bounds is your confidence interval. It tells you that you can be, for instance, 95% confident that the true population parameter falls within this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00e3c93-8cb5-4ba4-bbb5-a6abd3682557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a4ed8-2d73-4b2e-bde4-da3e8c68445b",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics and machine learning to estimate the sampling distribution of a statistic by repeatedly resampling from the original dataset. It helps in making inferences about population parameters, constructing confidence intervals, and assessing the variability of a statistic without relying on complex mathematical formulas or assumptions about the data distribution.\n",
    "\n",
    "Here are the key steps involved in the bootstrap method: \n",
    "\n",
    "\n",
    "\n",
    "Original Data: Start with your original dataset.\n",
    "\n",
    "\n",
    "Resampling: Create multiple \"fake\" datasets by randomly selecting data points from the original dataset with replacement. This means some data points may be picked more than once, while others may be left out.\n",
    "\n",
    "\n",
    "Statistic Calculation: For each of these fake datasets, calculate the statistic you're interested in (e.g., mean, median, etc.).\n",
    "\n",
    "\n",
    "Statistical Distribution: You now have a bunch of statistics, which form a distribution. This distribution represents the variability in the statistic due to random sampling.\n",
    "\n",
    "\n",
    "Confidence Interval: To create a confidence interval, find the range of values that includes a specified percentage (e.g., 95%) of the statistics. This is typically done by looking at percentiles of the distribution.\n",
    "\n",
    "\n",
    "Interpretation: The resulting range in the confidence interval tells you where you can reasonably expect the true value of the statistic to fall.\n",
    "\n",
    "\n",
    "Bootstrap is a valuable method because it helps estimate uncertainty in your data and statistics without needing to make strong assumptions about the data's distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431e244-f5ab-4d9f-80e4-cdd83bbabac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. A researcher wants to estimate the mean height of a population of trees. \n",
    "\n",
    "# They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. \n",
    "\n",
    "# Use bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75ef346a-b46a-4dc4-82e8-79993c0bc905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Mean Height: [13.96843381 16.10505736]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample Data\n",
    "sample_mean = 15  \n",
    "sample_std = 2    \n",
    "sample_size = 50  \n",
    "\n",
    "# Number Of Bootstrap Samples \n",
    "num_samples = 10000\n",
    "\n",
    "# Initialize An Array To Store Bootstrap Sample Means \n",
    "bootstrap_means = []\n",
    "\n",
    "# Perform Bootstrapping \n",
    "for _ in range(num_samples):\n",
    "    # Generate A Bootstrap Sample By Randomly Sampling With Replacement \n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_std, sample_size)\n",
    "    \n",
    "    # Calculate The Mean Of The Bootstrap Sample \n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    \n",
    "    # Store The Bootstrap Sample Mean \n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate The 95% Confidence Interval \n",
    "confidence_interval = np.percentile(bootstrap_means, [0, 100])\n",
    "\n",
    "print(\"95% Confidence Interval for Mean Height:\", confidence_interval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
