{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99825815-3b00-4f2d-a67b-fa29e72bba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Gradient Boosting Regression? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b506e7-3cfd-4a58-94d8-91d350b4e2f0",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks, which involve predicting a continuous numerical output variable based on input features. It is a type of ensemble learning method that combines the predictions of multiple weak learners, typically decision trees, to create a strong predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd57cda-e5be-4106-a9d0-6180085dc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. \n",
    "\n",
    "# Use a simple regression problem as an example and train the model on a small dataset. \n",
    "\n",
    "# Evaluate the model's performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0e6006f-6a04-4dee-8f86-05d88a01e84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.2939973248643864\n"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries \n",
    "import numpy as np \n",
    "from sklearn.datasets import fetch_california_housing \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Load The California Housing Prices Dataset \n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split The Data Into Training And Testing Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize The Gradient Boosting Regressor \n",
    "n_estimators = 100  # Number Of Trees (You Can Tune This) \n",
    "learning_rate = 0.1  # Shrinkage Parameter (You Can Tune This) \n",
    "model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, random_state=42)\n",
    "\n",
    "# Fit The Model To The Training Data \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions On The Test Data \n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate The Model \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88dd62b-693f-4cdc-b40e-bbd8747c91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. \n",
    "\n",
    "\n",
    "\n",
    "# Use grid search or random search to find the best hyperparameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b23300-cf3d-45cc-b579-52bfe92c5744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import fetch_california_housing \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV \n",
    "from sklearn.ensemble import GradientBoostingRegressor \n",
    "from sklearn.metrics import mean_squared_error \n",
    "\n",
    "# Load The California Housing Prices Dataset \n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target \n",
    "\n",
    "# Split The Data Into Training And Testing Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) \n",
    "\n",
    "# Define Hyperparameters Grid To Search \n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "} \n",
    "\n",
    "# Initialize The Gradient Boosting Regressor \n",
    "model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Perform Grid Search With Cross Validation\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get The Best Hyperparameters \n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params) \n",
    "\n",
    "# Fit The Model With The Best Hyperparameters To The Training Data \n",
    "best_model = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train) \n",
    "\n",
    "# Make Predictions On The Test Data \n",
    "y_pred = best_model.predict(X_test) \n",
    "\n",
    "# Evaluate The Model \n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error with Best Hyperparameters: {mse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2865235-c311-4690-b4d4-53bd81052138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431a037-31d4-465c-92ea-3177d8c09785",
   "metadata": {},
   "source": [
    "A weak learner in Gradient Boosting is a basic, simple model that performs slightly better than random chance on a given task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aee7602-fac5-46a9-8716-8bd49a246574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaea94d-3c78-4c18-8205-4aee478635cd",
   "metadata": {},
   "source": [
    "\n",
    "The intuition behind Gradient Boosting is that it combines multiple weak models (typically decision trees) sequentially to correct errors made by previous models, gradually improving prediction accuracy.\n",
    "\n",
    "\n",
    "It uses a gradient descent-like optimization process to adjust the model's predictions and combines their weighted results to make a final prediction. This process makes it a powerful and accurate machine learning algorithm.\n",
    " \n",
    "Sequential Learning: It builds models one at a time, with each new model correcting errors from the previous ones.\n",
    "\n",
    "\n",
    "Error Correction: Each new model focuses on the data points that previous models predicted incorrectly, improving overall accuracy.\n",
    "\n",
    "\n",
    "Gradient Descent: It uses a gradient descent-like optimization approach to minimize prediction errors and adjust model predictions in the right direction.\n",
    "\n",
    "\n",
    "Weighted Voting: The final prediction is a weighted sum of individual model predictions, where better models have a stronger influence.\n",
    "\n",
    "\n",
    "Regularization: Techniques like limiting tree depth and using a learning rate prevent overfitting and make the model robust.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0bb6b-ebd8-4a7b-a0f2-f201d07e6333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc23113-311c-4234-b686-09ac3752f8d9",
   "metadata": {},
   "source": [
    "Initialize with a Weak Learner: Gradient Boosting starts by creating an initial weak learner, often a shallow decision tree with a limited depth (few nodes). This first tree is referred to as the base learner or the first estimator.\n",
    "\n",
    "\n",
    "\n",
    "Calculate Residuals: After the first tree is built, the algorithm calculates the residuals (the differences between the actual target values and the predictions made by the first tree) for each training example.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Fit a New Weak Learner: The next step is to fit a new weak learner (tree) to the residuals. This new tree is constructed in a way that aims to reduce the residuals from the previous step. It focuses on the data points where the first tree performed poorly.\n",
    "\n",
    "\n",
    "Update Predictions: The predictions from the newly created tree are combined with the predictions of the previous trees. This combination is done by adding the predictions together (for regression tasks) or using weighted voting (for classification tasks).\n",
    "\n",
    "\n",
    "Repeat: Steps 2-4 are repeated iteratively. For each iteration, a new tree is constructed to correct the errors made by the current ensemble. The goal is to reduce the residuals further with each iteration.\n",
    "\n",
    "\n",
    "Regularization: To prevent overfitting, Gradient Boosting includes regularization techniques. Common regularization methods include controlling the maximum depth of individual trees, introducing a learning rate to shrink the contribution of each new tree, and adding randomization.\n",
    "\n",
    "\n",
    "Stopping Criteria: The iterative process continues until a specified number of trees (n_estimators) is reached or until some other stopping criteria are met, such as achieving satisfactory performance on a validation set.\n",
    "\n",
    "\n",
    "Final Ensemble: The final prediction is made by combining the predictions of all the trees in the ensemble, often using weighted voting or averaging.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6d4c3-16a2-44ac-9d47-cc60beff94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7b821-2f58-474e-8d5b-e732f9609ca1",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the underlying mathematical concepts and principles that drive its operation. Here are the key steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm:\n",
    "\n",
    "\n",
    "\n",
    "Start with an initial model's predictions.\n",
    "\n",
    "Calculate the errors (residuals) between these predictions and the actual data.\n",
    "\n",
    "Optimize a loss function by adjusting the predictions in the direction that reduces these errors.\n",
    "\n",
    "Repeat this process iteratively, adding new models that focus on correcting previous errors.\n",
    "\n",
    "Combine the predictions of all models to make the final prediction.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
