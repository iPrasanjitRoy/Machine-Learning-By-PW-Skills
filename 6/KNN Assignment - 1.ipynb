{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb62fce-d326-4471-919e-c7ae9f229639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49928bb-c3c4-4184-8249-36c54a6b0c00",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a machine learning algorithm that classifies or predicts data points based on their similarity to nearby data points in a training dataset. It's used for tasks like classification (assigning a category or label) and regression (predicting a value). KNN works by finding the K closest data points to a new data point and making predictions based on the majority class (for classification) or the average value (for regression) of those K neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5169b890-4374-4695-bba8-1febfdcc8a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How do you choose the value of K in KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09736fb3-99bc-4e0e-8418-bc7ee67ed0f9",
   "metadata": {},
   "source": [
    "To choose the value of K in K-Nearest Neighbors (KNN):\n",
    "\n",
    "Start with a small K and gradually increase it while testing the model's performance.\n",
    "\n",
    "Use techniques like cross-validation to evaluate K for different values.\n",
    "\n",
    "Consider domain knowledge and the dataset's characteristics to guide your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066310ab-6300-43d8-b6ce-88ad5dab4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40ff05d-55c5-48ce-a643-42175eeb9ec7",
   "metadata": {},
   "source": [
    "KNN classifier and KNN regressor are variants of the K-Nearest Neighbors algorithm that are used for different types of machine learning tasks. The classifier assigns discrete class labels, while the regressor predicts continuous numeric values. The choice between them depends on the nature of the problem you are trying to solve: classification or regression.\n",
    "\n",
    "\n",
    "KNN classifier is used for classification tasks. It assigns a class label or category to a data point based on the majority class among its K nearest neighbors.\n",
    "\n",
    "\n",
    "KNN regressor is used for regression tasks. It predicts a continuous numeric value for a data point based on the average (or weighted average) of the target values of its K nearest neighbors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1ebf5-a9b8-432d-9011-c4561d498e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How do you measure the performance of KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1e96f-a403-4b71-8d4a-f291fbc99097",
   "metadata": {},
   "source": [
    "# For KNN Classification:\n",
    "\n",
    "Accuracy: Measures the proportion of correctly classified data points.\n",
    "\n",
    "Confusion Matrix: Provides detailed information on true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "Precision: Calculates the accuracy of positive predictions.\n",
    "\n",
    "Recall (Sensitivity): Measures the ability to find all positive instances.\n",
    "\n",
    "F1-Score: Combines precision and recall for a balanced measure.\n",
    "\n",
    "ROC Curve and AUC: Evaluate the model's trade-off between true positives and false positives.\n",
    "\n",
    "Kappa Statistic: Measures the agreement between predicted and actual classifications.\n",
    "\n",
    "# For KNN Regression:\n",
    "\n",
    "Mean Absolute Error (MAE): Measures the average absolute difference between predicted and actual values.\n",
    "\n",
    "Mean Squared Error (MSE): Measures the average squared difference between predicted and actual values.\n",
    "\n",
    "Root Mean Squared Error (RMSE): A more interpretable metric derived from MSE.\n",
    "\n",
    "R-squared (R²): Quantifies how much variance in the target variable is explained by the model.\n",
    "\n",
    "Adjusted R-squared: Considers model complexity when assessing R².\n",
    "\n",
    "Residual Plots: Visual examination of differences between predicted and actual values for insights into model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9beb2aa-f53f-4fec-a97c-a16f4becc197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the curse of dimensionality in KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3334e4f-e076-470d-ba3d-d76c93f49d4c",
   "metadata": {},
   "source": [
    "The Curse of dimensionality is a term used in machine learning to describe the challenges and issues that arise when dealing with high-dimensional data in algorithms like K-Nearest Neighbors (KNN). It refers to the negative effects that occur as the number of dimensions (features or attributes) in a dataset increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315607c6-7ad1-43e9-ae79-90d1cd7e7cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How do you handle missing values in KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc532849-a71b-488b-8cba-8184ea3dfde4",
   "metadata": {},
   "source": [
    "## To handle missing values in K-Nearest Neighbors (KNN):\n",
    "\n",
    "Impute Values: Replace missing values with a measure like mean, median, or mode of the feature or use techniques like regression or KNN imputation.\n",
    "\n",
    "Delete Data: Remove data points (rows) with missing values or features (columns) with a high proportion of missing values.\n",
    "\n",
    "Transform Data: Treat missing values as a separate category or discretize continuous data into bins, including a bin for missing values.\n",
    "\n",
    "Model-Based Imputation: Use machine learning models to predict missing values based on available data.\n",
    "\n",
    "Special Consideration: Be cautious with distance-based KNN when imputing missing values to avoid altering distances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8065a6-bf17-4869-92f1-3998b52c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Compare and contrast the performance of the KNN classifier and regressor. \n",
    "\n",
    "# Which one is better for which type of problem ? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a232003-e7d1-480d-b977-c243b3d650a5",
   "metadata": {},
   "source": [
    "\n",
    "Choose a K-Nearest Neighbors (KNN) classifier when your problem involves discrete categories or classes, and you want to predict class labels. Use a KNN regressor when your problem requires predicting continuous numeric values, such as prices or quantities. \n",
    "\n",
    "\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Output Type: The primary distinction is in the type of output they provide: classifiers offer discrete class labels, whereas regressors offer continuous numeric values.\n",
    "\n",
    "\n",
    "Evaluation Metrics: Classifiers use classification-specific metrics, while regressors use regression-specific metrics. The choice of metrics reflects the nature of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7898d61e-cef9-4f03-a9bb-a3ccc81f20bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e72adb-6980-4315-9c26-4bed367025d4",
   "metadata": {},
   "source": [
    "# Strengths of KNN:\n",
    "\n",
    "# For Classification:\n",
    "\n",
    "Simplicity: KNN is easy to understand and implement, making it a good choice for quick prototyping.\n",
    "\n",
    "Non-parametric: It doesn't make strong assumptions about the data distribution, making it suitable for various types of datasets.\n",
    "\n",
    "Adaptability: KNN can adapt to changes in the data since it doesn't build a fixed model but relies on the local neighborhood.\n",
    "\n",
    "# For Regression:\n",
    "\n",
    "Versatility: KNN can handle regression tasks effectively by averaging the values of the nearest neighbors, making it flexible for a range of numeric prediction problems.\n",
    "\n",
    "Interpretability: KNN regression provides interpretable results, as the predictions are based on the actual values of neighboring data points.\n",
    "\n",
    "# Weaknesses of KNN:\n",
    "\n",
    "# For Classification:\n",
    "\n",
    "Computational Complexity: KNN can be computationally expensive, especially with large datasets, as it requires calculating distances to all data points.\n",
    "\n",
    "Sensitivity to Hyperparameters: The choice of K and the distance metric can significantly impact the model's performance, and there's no one-size-fits-all solution.\n",
    "\n",
    "Imbalanced Data: KNN may perform poorly on imbalanced datasets where one class dominates, as it tends to predict the majority class.\n",
    "\n",
    "# For Regression:\n",
    "\n",
    "Outliers: KNN regression is sensitive to outliers, as they can significantly affect the prediction since it relies on averaging nearby values.\n",
    "\n",
    "Local Sensitivity: KNN regression can be overly sensitive to the distribution of data in the local neighborhood, which might not represent the global data trends.\n",
    "\n",
    "# Addressing Weaknesses:\n",
    "\n",
    "To address the weaknesses of the KNN algorithm:\n",
    "\n",
    "Optimize K: Use techniques like cross-validation to find the optimal value of K for your specific dataset. Smaller K values can reduce noise, while larger K values provide smoother predictions.\n",
    "\n",
    "Distance Metrics: Experiment with different distance metrics (e.g., Euclidean, Manhattan, or custom distances) to find the one that best suits your data.\n",
    "\n",
    "Normalization/Standardization: Scale and standardize your features to ensure that all dimensions contribute equally to distance calculations.\n",
    "\n",
    "Feature Selection/Engineering: Choose relevant features and reduce dimensionality to improve the algorithm's efficiency and reduce sensitivity to irrelevant data.\n",
    "\n",
    "Outlier Handling: Detect and handle outliers in your data using techniques like robust statistics or outlier removal.\n",
    "\n",
    "Balanced Sampling: If dealing with imbalanced datasets, consider techniques like oversampling or undersampling to balance class distributions.\n",
    "\n",
    "Ensemble Methods: Combine KNN with other algorithms or ensemble methods to improve overall performance and robustness.\n",
    "\n",
    "Localized Models: Consider using variations of KNN like weighted KNN, kernelized KNN, or distance-weighted KNN to mitigate some of the algorithm's limitations.\n",
    "\n",
    "Efficient Data Structures: Use data structures like KD-Trees or Ball Trees to speed up nearest neighbor searches, especially for large datasets.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930ab418-d5e3-4995-af9b-06bdd1d02920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. What is the difference between Euclidean distance and Manhattan distance in KNN? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357f680-9d39-4ca6-aeba-598ab3136625",
   "metadata": {},
   "source": [
    "The main difference between Euclidean distance and Manhattan distance in KNN is the way they measure distance:\n",
    "\n",
    "Euclidean distance calculates the shortest straight-line distance between two points, allowing movement in any direction.\n",
    "\n",
    "\n",
    "\n",
    "Manhattan distance measures the distance along grid-like paths, considering only horizontal and vertical movements, like a taxi traveling along city streets.\n",
    "\n",
    "\n",
    "Euclidean distance is suitable for continuous data, while Manhattan distance is suitable for grid-like or discrete data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aaea52-f554-4efa-b295-715aee639792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. What is the role of feature scaling in KNN? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21380b03-9443-4969-88c1-97aa877f9f7b",
   "metadata": {},
   "source": [
    "The role of feature scaling in the K-Nearest Neighbors (KNN) algorithm is to ensure that all features or dimensions of the data contribute equally to the distance calculations. Feature scaling is crucial in KNN because the algorithm relies on measuring distances between data points to determine similarity, and features with different scales can lead to biased distance computations. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c737b40b-5d7b-43aa-b753-f64e272024d0",
   "metadata": {},
   "source": [
    "Common methods for feature scaling in KNN include:\n",
    "\n",
    "Min-Max Scaling (Normalization):\n",
    "\n",
    "Scales features to a specified range, often between 0 and 1.\n",
    "\n",
    "Z-score Standardization (Standard Scaling):\n",
    "\n",
    "Centers the data around a mean of 0 and scales it to have a standard deviation of 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
