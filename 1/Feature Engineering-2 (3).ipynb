{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0621b-566f-494c-8b5d-ba21d7fea83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Min-Max scaling, and how is it used in data preprocessing? \n",
    "\n",
    "# Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a395f8a-f2ad-4ea7-9130-2fd59ec623cb",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as feature scaling, is a data preprocessing technique used in machine learning and data analysis to transform numerical features so that they are within a specific range, typically between 0 and 1. \n",
    "\n",
    "This method is particularly useful when working with algorithms that are sensitive to the scale of the input features, such as distance-based algorithms (e.g., k-means clustering) and gradient-based optimization algorithms (e.g., neural networks).\n",
    "\n",
    "The formula to perform Min-Max scaling on a feature x is as follows:\n",
    "\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the original value of the feature.\n",
    "min(x) is the minimum value of the feature in the dataset.\n",
    "max(x) is the maximum value of the feature in the dataset.\n",
    "x_scaled is the scaled value of the feature within the range [0, 1].\n",
    "\n",
    "\n",
    "\n",
    "Here's an example to illustrate Min-Max scaling:\n",
    "\n",
    "Let's say you have a dataset of exam scores with scores ranging from 60 to 95. You want to scale these scores using Min-Max scaling.\n",
    "\n",
    "Original scores:\n",
    "\n",
    "Student 1: 60\n",
    "Student 2: 75\n",
    "Student 3: 85\n",
    "Student 4: 95\n",
    "First, calculate the minimum and maximum values:\n",
    "\n",
    "min(score) = 60\n",
    "max(score) = 95\n",
    "\n",
    "Now, apply the Min-Max scaling formula to each student's score:\n",
    "\n",
    "Student 1 scaled score = (60 - 60) / (95 - 60) = 0.0\n",
    "Student 2 scaled score = (75 - 60) / (95 - 60) = 0.375\n",
    "Student 3 scaled score = (85 - 60) / (95 - 60) = 0.625\n",
    "Student 4 scaled score = (95 - 60) / (95 - 60) = 1.0\n",
    "\n",
    "\n",
    "\n",
    "After Min-Max scaling, the scores are transformed to the range [0, 1] while preserving their relative differences.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb39cfc-fa56-45fc-af4f-55187aa37196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "\n",
    "# Provide an example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79faf346-d07d-422f-893c-62a1a575ee5a",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as Normalization, is another data preprocessing technique used to scale numerical features.\n",
    "\n",
    "\n",
    "Mathematically, the normalization of a feature vector x is performed as follows:\n",
    "\n",
    "x_normalized = x / ||x||\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the original feature vector.\n",
    "||x|| represents the Euclidean norm of the vector, calculated as the square root of the sum of squared elements.\n",
    "\n",
    "Unlike Min-Max scaling that scales features within a specific range (e.g., [0, 1]), the Unit Vector technique scales features to have a unit norm, meaning that the vector representing the feature values will have a length of 1 while maintaining the direction of the original vector.\n",
    "\n",
    "\n",
    "Here's an example to illustrate the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two features representing height in centimeters and weight in kilograms:\n",
    "\n",
    "Original data:\n",
    "\n",
    "Person 1: Height = 160 cm, Weight = 55 kg\n",
    "Person 2: Height = 175 cm, Weight = 70 kg\n",
    "Person 3: Height = 180 cm, Weight = 65 kg\n",
    "Let's calculate the Euclidean norms for each person's feature vector:\n",
    "\n",
    "Person 1: ||x1|| = sqrt(160^2 + 55^2) ≈ 167.91\n",
    "Person 2: ||x2|| = sqrt(175^2 + 70^2) ≈ 188.46\n",
    "Person 3: ||x3|| = sqrt(180^2 + 65^2) ≈ 193.29\n",
    "Now, normalize each feature vector:\n",
    "\n",
    "Person 1 normalized: (160 / 167.91, 55 / 167.91) ≈ (0.953, 0.302)\n",
    "Person 2 normalized: (175 / 188.46, 70 / 188.46) ≈ (0.928, 0.372)\n",
    "Person 3 normalized: (180 / 193.29, 65 / 193.29) ≈ (0.932, 0.336)\n",
    "In this case, the normalization process ensures that the length of each feature vector is 1, while the relative relationships between height and weight for each person are maintained.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In summary, the key difference between Min-Max scaling and the Unit Vector technique (Normalization) lies in their objectives. Min-Max scaling scales features to a specific range, while the Unit Vector technique normalizes features to have a unit norm, maintaining their direction and ensuring equal contribution to distance calculations. The choice between these techniques depends on the requirements of the specific machine learning algorithm you're using and the nature of your data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e501d-3608-4967-999a-0cd7e8eebe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? \n",
    "\n",
    "# Provide an  example to illustrate its application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac3639e-ed75-4e11-b86e-e76b91fcf442",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in the field of machine learning and statistics to transform high-dimensional data into a lower-dimensional representation while retaining as much of the original data's variability as possible. PCA achieves this by identifying the principal components, which are orthogonal (uncorrelated) linear combinations of the original features that capture the most significant variations in the data.\n",
    "\n",
    "\n",
    "he primary steps of PCA are as follows:\n",
    "\n",
    "Standardize the Data: Before performing PCA, it's common to standardize the data by subtracting the mean and dividing by the standard deviation of each feature. This ensures that each feature contributes equally to the analysis.\n",
    "\n",
    "Compute Covariance Matrix: The covariance matrix is computed based on the standardized data. It shows how features vary together.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Eigenvectors and eigenvalues are calculated from the covariance matrix. Eigenvectors represent the directions of maximum variance, and eigenvalues indicate the magnitude of variance in those directions.\n",
    "\n",
    "Sort Eigenvectors: Eigenvectors are sorted based on their corresponding eigenvalues in decreasing order. The eigenvector with the highest eigenvalue represents the first principal component, the second highest represents the second principal component, and so on.\n",
    "\n",
    "Select Principal Components: You can choose a subset of the top eigenvectors (principal components) that capture a desired amount of variance. This determines the dimensionality of the reduced space.\n",
    "\n",
    "Transform Data: The original data is transformed into the new reduced-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "Here's an example to illustrate PCA's application:\n",
    "\n",
    "Suppose you have a dataset of 2D points in space:\n",
    "\n",
    "Original data:\n",
    "\n",
    "Point 1: (2, 3)\n",
    "Point 2: (5, 7)\n",
    "Point 3: (8, 10)\n",
    "Point 4: (11, 14)\n",
    "Standardize the Data: If necessary, you'd standardize the data by subtracting the mean and dividing by the standard deviation of each dimension.\n",
    "\n",
    "Compute Covariance Matrix: Compute the covariance matrix based on the standardized data.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort Eigenvectors: Assume the eigenvectors are sorted as [eig1, eig2], where eig1 has the higher eigenvalue.\n",
    "\n",
    "Select Principal Components: Let's say you decide to keep only the first principal component (eig1).\n",
    "\n",
    "Transform Data: Project the original data onto the first principal component.\n",
    "\n",
    "After the transformation, your data would now be represented along a single dimension, capturing the most significant variance. This lower-dimensional representation is the result of PCA's dimensionality reduction process.\n",
    "\n",
    "In practice, PCA can be particularly useful for visualization, noise reduction, and improving the efficiency of machine learning algorithms by reducing the number of features while retaining the most important information.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8940cd86-d19f-45c0-9e92-fb9d4bb7e5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? \n",
    "\n",
    "\n",
    "# Provide an example to illustrate this concept \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71232943-b69b-44e8-9ea9-a17e1476da79",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) and Feature Extraction are closely related concepts, with PCA being a technique commonly used for feature extraction. Feature extraction involves transforming the original features of a dataset into a new set of features that capture the most important information, while reducing dimensionality. PCA achieves this by identifying the principal components, which are linear combinations of the original features that capture the most significant variability in the data.\n",
    "\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. Original Data: Consider a dataset with multiple features, which might represent various measurements or attributes of the data instances.\n",
    "\n",
    "2. Standardize the Data: If necessary, standardize the data to ensure all features have comparable scales.\n",
    "\n",
    "3. Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "4. Compute Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. Sort Eigenvectors: Sort the eigenvectors in decreasing order based on their corresponding eigenvalues.\n",
    "\n",
    "6. Select Principal Components: Choose a subset of the top eigenvectors (principal components) that capture a desired amount of variance. These selected components will be used as the new features.\n",
    "\n",
    "7. Transform Data: Transform the original data into the reduced-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "1. Original Data: Consider a dataset with multiple features, which might represent various measurements or attributes of the data instances.\n",
    "\n",
    "2. Standardize the Data: If necessary, standardize the data to ensure all features have comparable scales.\n",
    "\n",
    "3. Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "4. Compute Eigenvectors and Eigenvalues: Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "5. Sort Eigenvectors: Sort the eigenvectors in decreasing order based on their corresponding eigenvalues.\n",
    "\n",
    "6. Select Principal Components: Choose a subset of the top eigenvectors (principal components) that capture a desired amount of variance. These selected components will be used as the new features.\n",
    "\n",
    "7. Transform Data: Transform the original data into the reduced-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "Here's an example to illustrate how PCA is used for feature extraction:\n",
    "\n",
    "Original Data: Suppose you have a dataset with three features: height, weight, and age, of individuals.\n",
    "\n",
    "Standardize the Data: Standardize the data to ensure all features have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort Eigenvectors: Assume the sorted eigenvectors are [eig1, eig2, eig3], with eig1 having the highest eigenvalue.\n",
    "\n",
    "Select Principal Components: Decide to keep only the first two eigenvectors (eig1 and eig2), capturing a significant portion of the variance.\n",
    "\n",
    "Transform Data: Project the original data onto the first two principal components.\n",
    "\n",
    "In this case, the original three features have been transformed into a new two-dimensional space represented by the first two principal components. These new features, derived from the eigenvectors, capture the most important information from the original features while reducing dimensionality. The transformed data can now be used for analysis or machine learning tasks.\n",
    "\n",
    "Feature extraction through PCA can be especially beneficial when working with high-dimensional datasets, as it allows you to represent the data in a more compact form while preserving the key characteristics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de805048-e00c-4440-b392-bfcd65d398dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. \n",
    "\n",
    "# The dataset contains features such as price, rating, and delivery time. \n",
    "\n",
    "#  Explain how you would use Min-Max scaling to preprocess the data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2957565-6206-4927-8425-3862f583e4ff",
   "metadata": {},
   "source": [
    "In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess the data and ensure that the features are within a common range. This is important because recommendation systems often rely on similarity or distance measures between items, and having features on a consistent scale can improve the accuracy of the recommendations.\n",
    "\n",
    "Here's how you would use Min-Max scaling to preprocess the features in your dataset:\n",
    "\n",
    "Understand the Features: Review the features available in your dataset, which in this case are price, rating, and delivery time. Understand the range and distribution of each feature.\n",
    "\n",
    "Calculate Min and Max: For each feature, calculate the minimum and maximum values present in the dataset. For example, let's say:\n",
    "\n",
    "Price: Minimum = $5, Maximum = $30\n",
    "Rating: Minimum = 2.0, Maximum = 5.0\n",
    "Delivery Time: Minimum = 15 minutes, Maximum = 60 minutes\n",
    "\n",
    "\n",
    "Apply Min-Max Scaling: For each feature, apply the Min-Max scaling formula to transform the values to the [0, 1] range:\n",
    "\n",
    "\n",
    "x_scaled = (x - min(x)) / (max(x) - min(x))\n",
    "\n",
    "Scaled Price = (Price - $5) / ($30 - $5)\n",
    "Scaled Rating = (Rating - 2.0) / (5.0 - 2.0)\n",
    "Scaled Delivery Time = (Delivery Time - 15) / (60 - 15)\n",
    "\n",
    "Preprocessed Data: After applying Min-Max scaling, your dataset's features will now be in the [0, 1] range:\n",
    "\n",
    "Scaled Price: 0.0 to 1.0\n",
    "Scaled Rating: 0.0 to 1.0\n",
    "Scaled Delivery Time: 0.0 to 1.0\n",
    "\n",
    "Use Preprocessed Data: The preprocessed data with scaled features can now be used to build your recommendation system. When calculating similarities or distances between items (food options in this case), the consistent scaling ensures that no feature dominates others due to its original scale.\n",
    "\n",
    "By using Min-Max scaling, you have transformed the features in a way that maintains their relative relationships while bringing them to a common scale. This preprocessing step can contribute to the accuracy and effectiveness of your recommendation system by enabling more meaningful comparisons between different food options based on their features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06492860-26a1-4c86-a0c1-7bba9c964e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. \n",
    "\n",
    "# The dataset contains many features, such as company financial data and market trends. \n",
    "\n",
    "# Explain how you would use PCA to reduce the dimensionality of the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3beb10-a10d-454b-9731-3230e1218851",
   "metadata": {},
   "source": [
    "When working with a dataset that contains numerous features, such as company financial data and market trends, using PCA (Principal Component Analysis) can be a valuable technique to reduce the dimensionality of the data while retaining the most important information. Reducing dimensionality can help in simplifying the model, improving its efficiency, and potentially mitigating the curse of dimensionality. Here's how you could use PCA for this stock price prediction project:\n",
    "\n",
    "\n",
    "Understand the Dataset: First, gain a thorough understanding of the dataset's features, which might include financial indicators (e.g., revenue, profit, debt), market indicators (e.g., market sentiment, interest rates), and potentially other external factors affecting stock prices.\n",
    "\n",
    "Data Preprocessing: Standardize the data to ensure that all features have comparable scales. This step is important for PCA as it's sensitive to the scale of the features.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data. The covariance matrix provides insight into how features are correlated and how they vary together.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors represent the directions of maximum variance in the data.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors in decreasing order of their corresponding eigenvalues. The eigenvectors with higher eigenvalues capture more variance and are considered the principal components.\n",
    "\n",
    "Select Principal Components: Decide on the number of principal components you want to retain. This decision can be based on the cumulative explained variance, where you aim to retain a certain percentage of the total variance in the data. This determines how many dimensions your reduced dataset will have.\n",
    "\n",
    "Transform Data: Project the original standardized data onto the selected principal components. This results in a new dataset with reduced dimensions, as the principal components serve as the new features.\n",
    "\n",
    "Build Prediction Model: Use the transformed dataset, which has reduced dimensionality, to build your stock price prediction model. You can use various machine learning algorithms for regression, taking advantage of the simplified dataset.\n",
    "\n",
    "\n",
    "It's important to note that while PCA reduces dimensionality, it comes with the trade-off of losing some interpretability of the individual features. However, the retained principal components are combinations of the original features that capture the most significant variability in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475ea4d-8aed-413b-a35b-1bb27e484177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8106d-01ac-457d-bc57-ef1a80bd8c6f",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling on a dataset and transform the values to a range of -1 to 1, you can use the following formula:\n",
    "\n",
    "\n",
    "x_scaled = 2 * (x - min(x)) / (max(x) - min(x)) - 1\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "x is the original value.\n",
    "min(x) is the minimum value in the dataset.\n",
    "max(x) is the maximum value in the dataset.\n",
    "x_scaled is the scaled value within the range of -1 to 1.\n",
    "\n",
    "\n",
    "Let's apply this formula to your dataset: [1, 5, 10, 15, 20]\n",
    "\n",
    "min(x) = 1\n",
    "max(x) = 20\n",
    "\n",
    "Now, let's calculate the scaled values for each element:\n",
    "\n",
    "For x = 1: \n",
    "x_scaled = 2 * (1 - 1) / (20 - 1) - 1 = -1\n",
    "\n",
    "\n",
    "For x = 5: \n",
    "x_scaled = 2 * (5 - 1) / (20 - 1) - 1 = -0.5\n",
    "\n",
    "\n",
    "For x = 10: \n",
    "x_scaled = 2 * (10 - 1) / (20 - 1) - 1 = 0\n",
    "\n",
    "\n",
    "After applying Min-Max scaling with the specified formula, the dataset [1, 5, 10, 15, 20] is transformed to the range of -1 to 1 as follows:\n",
    "\n",
    "Scaled dataset: [-1, -0.5, 0, 0.5, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d88f7ad-90dd-464d-a431-d7ddf87a268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: [ 1  5 10 15 20]\n",
      "Scaled dataset: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "dataset = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Min-Max scaling\n",
    "min_val = dataset.min()\n",
    "max_val = dataset.max()\n",
    "\n",
    "scaled_dataset = 2 * (dataset - min_val) / (max_val - min_val) - 1\n",
    "\n",
    "print(\"Original dataset:\", dataset)\n",
    "print(\"Scaled dataset:\", scaled_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6811983-d30e-44d9-a641-8698fc48830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. \n",
    "\n",
    "# How many principal components would you choose to retain, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1521178c-508a-48b1-b043-9ec082379920",
   "metadata": {},
   "source": [
    "Choosing the number of principal components to retain in PCA depends on the variance explained by those components and the desired level of dimensionality reduction. \n",
    "\n",
    "Let's break down the steps to determine how many principal components to retain for the given dataset with features: [height, weight, age, gender, blood pressure].\n",
    "\n",
    "Standardize the Data: Start by standardizing the data so that all features have comparable scales. This step is crucial for PCA.\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix based on the standardized data.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Calculate the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors in decreasing order based on their corresponding eigenvalues.\n",
    "\n",
    "Explained Variance: Calculate the explained variance ratio for each principal component. This ratio tells you the proportion of the total variance in the data that is captured by each component.\n",
    "\n",
    "Cumulative Explained Variance: Calculate the cumulative explained variance by summing up the explained variance ratios. This will help you understand how much variance is retained when considering a certain number of principal components.\n",
    "\n",
    "Choose Number of Components: Based on the cumulative explained variance, you can choose the number of principal components that retain a desired amount of variance. A common approach is to retain components that collectively capture a high percentage (e.g., 95% or more) of the total variance.\n",
    "\n",
    "The decision of how many principal components to retain depends on your specific goals and the trade-off between dimensionality reduction and preserving variance. Retaining a higher number of components retains more information but might lead to a higher-dimensional transformed dataset. On the other hand, retaining fewer components simplifies the dataset but might lose some information.\n",
    "\n",
    "\n",
    "It's worth mentioning that in the context of the given features [height, weight, age, gender, blood pressure], some of these features might be less relevant for dimensionality reduction. For instance, \"gender\" might not contribute significantly to variance compared to continuous numerical features like \"height,\" \"weight,\" and \"blood pressure.\"\n",
    "\n",
    "\n",
    "In summary, the choice of how many principal components to retain involves a balance between retaining variance and reducing dimensionality while considering the specific characteristics of your dataset and the goals of your analysis.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a02da247-1e62-4225-9bc6-ac035690fe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data:\n",
      "[[ 1.00406417e+00  6.70269784e-01  1.77664948e-01  8.86256479e-17]\n",
      " [-1.23315205e+00 -5.26264780e-01  2.57265124e-01  8.86256479e-17]\n",
      " [ 2.97898198e+00 -2.99876773e-01 -1.84530372e-01  8.86256479e-17]\n",
      " [-2.74989410e+00  1.55871769e-01 -2.50399700e-01  8.86256479e-17]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Example dataset with features [height, weight, age, gender, blood pressure]\n",
    "dataset = np.array([\n",
    "    [170, 65, 30, 1, 120],\n",
    "    [165, 55, 25, 0, 110],\n",
    "    [180, 75, 40, 1, 130],\n",
    "    [155, 45, 22, 0, 100],\n",
    "])\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Perform PCA with 2 components (you can choose the number of components)\n",
    "num_components = 4\n",
    "pca = PCA(n_components=num_components)\n",
    "pca_result = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"Transformed data:\")\n",
    "print(pca_result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd535d-4842-46a6-aea7-fffee488e56e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
